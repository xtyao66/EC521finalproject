{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('dataset_phishing.csv')\n",
    "y = data['status'].apply(lambda x: 1 if x == 'phishing' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 URL-related features:\n",
      "Feature: ://ww, Importance: 0.014676138617375406\n",
      "Feature: ww., Importance: 0.0138751939602062\n",
      "Feature: /www., Importance: 0.013833184108200869\n",
      "Feature: ://w, Importance: 0.01328564378334851\n",
      "Feature: www, Importance: 0.013050248763892288\n",
      "Feature: //w, Importance: 0.012642878990259584\n",
      "Feature: //www, Importance: 0.01159382382023095\n",
      "Feature: www., Importance: 0.010380559015645847\n",
      "Feature: /w, Importance: 0.01002457753068443\n",
      "Feature: //ww, Importance: 0.009421645662841923\n",
      "Feature: /ww, Importance: 0.00823598268546701\n",
      "Feature: /www, Importance: 0.00818132025993299\n",
      "Feature: w., Importance: 0.007420459138158683\n",
      "Feature: ww, Importance: 0.006659957676010697\n",
      "Feature: pp, Importance: 0.00472886712080822\n",
      "Feature: wp-, Importance: 0.004464075251524952\n",
      "Feature: s://w, Importance: 0.003607418858691249\n",
      "Feature: //, Importance: 0.003516320254046212\n",
      "Feature: iki, Importance: 0.003492515067564495\n",
      "Feature: tt, Importance: 0.003417436434608161\n",
      "Top five features (excluding url-related features):\n",
      "Feature: google_index, Importance: 0.08266812530483551\n",
      "Feature: page_rank, Importance: 0.05626272975596572\n",
      "Feature: nb_hyperlinks, Importance: 0.03965047087870919\n",
      "Feature: web_traffic, Importance: 0.027900144512420053\n"
     ]
    }
   ],
   "source": [
    "all_features = data.columns.tolist()\n",
    "all_features.remove('status') \n",
    "# tf-idf embedding for url\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# extract numerical features\n",
    "numerical_features = all_features.copy()\n",
    "numerical_features.remove('url')  \n",
    "X_numerical = data[numerical_features].values\n",
    "\n",
    "# combine url with rest featrues and split\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# use random forest to rate feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# find top 20 important features in url\n",
    "url_feature_importances = feature_importances[:len(tfidf_feature_names)]\n",
    "url_features_with_importance = list(zip(tfidf_feature_names, url_feature_importances))\n",
    "top_ten_url_features = sorted(url_features_with_importance, key=lambda x: x[1], reverse=True)[:20]\n",
    "print(\"Top 20 URL-related features:\")\n",
    "for feature, importance in top_ten_url_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# find top 5 important features except url\n",
    "remaining_feature_importances = feature_importances[len(tfidf_feature_names):]\n",
    "remaining_feature_names = numerical_features\n",
    "remaining_features_with_importance = list(zip(remaining_feature_names, remaining_feature_importances))\n",
    "top_five_features = sorted(remaining_features_with_importance, key=lambda x: x[1], reverse=True)[:4]\n",
    "print(\"Top five features (excluding url-related features):\")\n",
    "for feature, importance in top_five_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and label extraction, including additional numerical features\n",
    "# selected_features = [ 'length_url', 'nb_dots', 'https_token', 'nb_subdomains']  # Add other selected feature column names\n",
    "selected_features = [item[0] for item in top_five_features]\n",
    "X_numerical = data[selected_features].values  # Extract numerical features\n",
    "\n",
    "# Character n-gram TF-IDF vectorization for the 'url' feature\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "\n",
    "# top_features_names = [feature[0] for feature in top_ten_url_features]\n",
    "# feature_indices = [list(tfidf_feature_names).index(feature) for feature in top_features_names]\n",
    "# feature_values = X_tfidf[:, feature_indices]\n",
    "\n",
    "# Combine TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "# X_combined = np.hstack((feature_values, X_numerical))\n",
    "\n",
    "# Fit the scaler only on the training data and transform both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.proj = nn.Linear(feature_dim, 64)\n",
    "        self.out = nn.Linear(64, feature_dim)  # Ensure output dimension matches feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        eij = self.proj(x)\n",
    "        eij = torch.tanh(eij)\n",
    "        eij = self.out(eij)\n",
    "        a = torch.softmax(eij, dim=1)\n",
    "        weighted_input = x * a\n",
    "        return weighted_input  # Return the weighted input for further processing\n",
    "\n",
    "class AdvancedURLNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedURLNetWithAttention, self).__init__()\n",
    "        # Increased the complexity of the first layer and added a second attention layer\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Reduced dropout\n",
    "        self.attention1 = Attention(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.attention2 = Attention(512)  # New attention layer\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = AdvancedURLNetWithAttention(X_train_tensor.shape[1])  # assuming this class is defined correctly\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    if cuda_available:\n",
    "        model = model.cuda()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU\n",
    "            all_labels.extend(labels.cpu().numpy())    # Move labels back to CPU\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.3550191819667816\n",
      "Epoch 6/100, Loss: 0.02074892818927765\n",
      "Epoch 11/100, Loss: 0.01348068006336689\n",
      "Epoch 16/100, Loss: 0.0057718646712601185\n",
      "Epoch 21/100, Loss: 0.0004906795220449567\n",
      "Epoch 26/100, Loss: 0.004253881983458996\n",
      "Epoch 31/100, Loss: 0.0006040992448106408\n",
      "Epoch 36/100, Loss: 0.0004950417787767947\n",
      "Epoch 41/100, Loss: 0.0001622400595806539\n",
      "Epoch 46/100, Loss: 0.0001774134289007634\n",
      "Epoch 51/100, Loss: 0.0050127157010138035\n",
      "Epoch 56/100, Loss: 7.788266520947218e-05\n",
      "Epoch 61/100, Loss: 0.00012811714259441942\n",
      "Epoch 66/100, Loss: 0.00020415164181031287\n",
      "Epoch 71/100, Loss: 0.00041952033643610775\n",
      "Epoch 76/100, Loss: 0.00015996901493053883\n",
      "Epoch 81/100, Loss: 0.00015720243391115218\n",
      "Epoch 86/100, Loss: 0.0013190966565161943\n",
      "Epoch 91/100, Loss: 0.0002440294629195705\n",
      "Epoch 96/100, Loss: 0.00038085426785983145\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9601924759405074, Precision: 0.9609236234458259, Recall: 0.95837023914969, F1-Score: 0.9596452328159645\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45720\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 11430",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Example usage for a single URL and its numerical features\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m single_features \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mhttp://www.crestonwood.com/router.php\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m35\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m]  \u001b[39m# URL followed by its numerical features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m prediction \u001b[39m=\u001b[39m transform_and_predict(single_features, vectorizer, scaler, model)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPhishing\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m prediction \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mLegitimate\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(numerical_features\u001b[39m.\u001b[39msize)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Combine TF-IDF features with other numerical features\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m combined_features \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mhstack((tfidf_features, numerical_features))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Standardize the features using the pre-fitted scaler\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/ubuntu/yibo/EC521finalproject/model_multi.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m scaled_features \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(combined_features)  \u001b[39m# No need for additional brackets now\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/503/lib/python3.9/site-packages/numpy/core/shape_base.py:370\u001b[0m, in \u001b[0;36mhstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39mdtype, casting\u001b[39m=\u001b[39mcasting)\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, casting\u001b[39m=\u001b[39;49mcasting)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 1 and the array at index 1 has size 11430"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def transform_and_predict(features_list, vectorizer, scaler, model):\n",
    "    # Separate the URL from the numerical features\n",
    "    url, numerical_features = features_list[0], np.array(features_list[1:])\n",
    "\n",
    "    # Transform the URL using the pre-fitted TF-IDF vectorizer\n",
    "    tfidf_features = vectorizer.transform([url]).toarray()\n",
    "\n",
    "    # Ensure numerical_features is a 2D array with a single sample\n",
    "    numerical_features = numerical_features.reshape(1, -1)\n",
    "\n",
    "    # Combine TF-IDF features with other numerical features\n",
    "    combined_features = np.hstack((tfidf_features, numerical_features))\n",
    "\n",
    "    # Standardize the features using the pre-fitted scaler\n",
    "    scaled_features = scaler.transform(combined_features)  # No need for additional brackets now\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "\n",
    "    # Predict using the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features_tensor)\n",
    "        predicted_class = (prediction.squeeze() > 0.5).float()\n",
    "        return predicted_class.item()  # Return the prediction as a Python scalar\n",
    "\n",
    "# Example usage for a single URL and its numerical features\n",
    "single_features = [\"http://www.crestonwood.com/router.php\", 35, 3, 1, 2]  # URL followed by its numerical features\n",
    "prediction = transform_and_predict(single_features, vectorizer, scaler, model)\n",
    "print(\"Phishing\" if prediction == 1 else \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scaler.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the PyTorch model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "# Save the fitted TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the fitted scaler\n",
    "joblib.dump(scaler, 'standard_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
