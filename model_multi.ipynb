{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tldextract\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('dataset_phishing.csv')\n",
    "\n",
    "# Feature and label extraction, including additional numerical features\n",
    "selected_features = ['url', 'length_url', 'nb_dots', 'https_token', 'nb_subdomains']  # Add other selected feature column names\n",
    "X_numerical = data[selected_features[1:]].values  # Extract numerical features\n",
    "y = data['status'].apply(lambda x: 1 if x == 'phishing' else 0).values\n",
    "\n",
    "# Character n-gram TF-IDF vectorization for the 'url' feature\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "\n",
    "# Combine TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "\n",
    "# Fit the scaler only on the training data and transform both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.proj = nn.Linear(feature_dim, 64)\n",
    "        self.out = nn.Linear(64, feature_dim)  # Ensure output dimension matches feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        eij = self.proj(x)\n",
    "        eij = torch.tanh(eij)\n",
    "        eij = self.out(eij)\n",
    "        a = torch.softmax(eij, dim=1)\n",
    "        weighted_input = x * a\n",
    "        return weighted_input  # Return the weighted input for further processing\n",
    "\n",
    "class AdvancedURLNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedURLNetWithAttention, self).__init__()\n",
    "        # Increased the complexity of the first layer and added a second attention layer\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Reduced dropout\n",
    "        self.attention1 = Attention(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.attention2 = Attention(512)  # New attention layer\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = AdvancedURLNetWithAttention(X_train_tensor.shape[1])  # assuming this class is defined correctly\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    if cuda_available:\n",
    "        model = model.cuda()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU\n",
    "            all_labels.extend(labels.cpu().numpy())    # Move labels back to CPU\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.35087350010871887\n",
      "Epoch 6/100, Loss: 0.0742310881614685\n",
      "Epoch 11/100, Loss: 0.033635057508945465\n",
      "Epoch 16/100, Loss: 0.028876053169369698\n",
      "Epoch 21/100, Loss: 0.08816889673471451\n",
      "Epoch 26/100, Loss: 0.0003738495579455048\n",
      "Epoch 31/100, Loss: 0.004314770922064781\n",
      "Epoch 36/100, Loss: 0.015597413294017315\n",
      "Epoch 41/100, Loss: 0.004892611410468817\n",
      "Epoch 46/100, Loss: 0.00022619387891609222\n",
      "Epoch 51/100, Loss: 0.0002798225323203951\n",
      "Epoch 56/100, Loss: 0.00036960034049116075\n",
      "Epoch 61/100, Loss: 0.0008046565926633775\n",
      "Epoch 66/100, Loss: 0.000380099838366732\n",
      "Epoch 71/100, Loss: 0.0001737980346661061\n",
      "Epoch 76/100, Loss: 0.011571326293051243\n",
      "Epoch 81/100, Loss: 0.0006883034948259592\n",
      "Epoch 86/100, Loss: 0.0014102268032729626\n",
      "Epoch 91/100, Loss: 0.0008534871158190072\n",
      "Epoch 96/100, Loss: 0.0002813483006320894\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9190726159230096, Precision: 0.9206773618538324, Recall: 0.9149689991142604, F1-Score: 0.9178143047534429\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def transform_and_predict(features_list, vectorizer, scaler, model):\n",
    "    # Separate the URL from the numerical features\n",
    "    url, numerical_features = features_list[0], np.array(features_list[1:])\n",
    "\n",
    "    # Transform the URL using the pre-fitted TF-IDF vectorizer\n",
    "    tfidf_features = vectorizer.transform([url]).toarray()\n",
    "\n",
    "    # Ensure numerical_features is a 2D array with a single sample\n",
    "    numerical_features = numerical_features.reshape(1, -1)\n",
    "\n",
    "    # Combine TF-IDF features with other numerical features\n",
    "    combined_features = np.hstack((tfidf_features, numerical_features))\n",
    "\n",
    "    # Standardize the features using the pre-fitted scaler\n",
    "    scaled_features = scaler.transform(combined_features)  # No need for additional brackets now\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "\n",
    "    # Predict using the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features_tensor)\n",
    "        predicted_class = (prediction.squeeze() > 0.5).float()\n",
    "        return predicted_class.item()  # Return the prediction as a Python scalar\n",
    "\n",
    "# Example usage for a single URL and its numerical features\n",
    "single_features = [\"http://www.crestonwood.com/router.php\", 35, 3, 1, 2]  # URL followed by its numerical features\n",
    "prediction = transform_and_predict(single_features, vectorizer, scaler, model)\n",
    "print(\"Phishing\" if prediction == 1 else \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scaler.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the PyTorch model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "# Save the fitted TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the fitted scaler\n",
    "joblib.dump(scaler, 'standard_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
