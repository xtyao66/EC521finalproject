{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('dataset_phishing.csv')\n",
    "y = data['status'].apply(lambda x: 1 if x == 'phishing' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five features (excluding url-related features):\n",
      "Feature: google_index, Importance: 0.08119731915700858\n",
      "Feature: page_rank, Importance: 0.054764258448125436\n",
      "Feature: nb_hyperlinks, Importance: 0.04117338134687459\n",
      "Feature: web_traffic, Importance: 0.03248001035621982\n",
      "Feature: domain_age, Importance: 0.020893432999659213\n"
     ]
    }
   ],
   "source": [
    "all_features = data.columns.tolist()\n",
    "all_features.remove('status') \n",
    "# tf-idf embedding for url\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# extract numerical features\n",
    "numerical_features = all_features.copy()\n",
    "numerical_features.remove('url')  \n",
    "X_numerical = data[numerical_features].values\n",
    "\n",
    "# combine url with rest featrues and split\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# use random forest to rate feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# find top 20 important features in url\n",
    "url_feature_importances = feature_importances[:len(tfidf_feature_names)]\n",
    "url_features_with_importance = list(zip(tfidf_feature_names, url_feature_importances))\n",
    "top_ten_url_features = sorted(url_features_with_importance, key=lambda x: x[1], reverse=True)[:20]\n",
    "# print(\"Top 20 URL-related features:\")\n",
    "# for feature, importance in top_ten_url_features:\n",
    "#     print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# find top 5 important features except url\n",
    "remaining_feature_importances = feature_importances[len(tfidf_feature_names):]\n",
    "remaining_feature_names = numerical_features\n",
    "remaining_features_with_importance = list(zip(remaining_feature_names, remaining_feature_importances))\n",
    "top_five_features = sorted(remaining_features_with_importance, key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top five features (excluding url-related features):\")\n",
    "for feature, importance in top_five_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and label extraction, including additional numerical features\n",
    "selected_features = [ 'length_url', 'nb_dots', 'https_token', 'nb_subdomains', 'google_index', 'nb_hyperlinks']  # Add other selected feature column names\n",
    "# selected_features = [item[0] for item in top_five_features]\n",
    "X_numerical = data[selected_features].values  # Extract numerical features\n",
    "\n",
    "# Character n-gram TF-IDF vectorization for the 'url' feature\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "\n",
    "# top_features_names = [feature[0] for feature in top_ten_url_features]\n",
    "# feature_indices = [list(tfidf_feature_names).index(feature) for feature in top_features_names]\n",
    "# feature_values = X_tfidf[:, feature_indices]\n",
    "\n",
    "# Combine TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "# X_combined = np.hstack((feature_values, X_numerical))\n",
    "\n",
    "# Fit the scaler only on the training data and transform both sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.proj = nn.Linear(feature_dim, 64)\n",
    "        self.out = nn.Linear(64, feature_dim)  # Ensure output dimension matches feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        eij = self.proj(x)\n",
    "        eij = torch.tanh(eij)\n",
    "        eij = self.out(eij)\n",
    "        a = torch.softmax(eij, dim=1)\n",
    "        weighted_input = x * a\n",
    "        return weighted_input  # Return the weighted input for further processing\n",
    "\n",
    "class AdvancedURLNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedURLNetWithAttention, self).__init__()\n",
    "        # Increased the complexity of the first layer and added a second attention layer\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Reduced dropout\n",
    "        self.attention1 = Attention(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.attention2 = Attention(512)  # New attention layer\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = AdvancedURLNetWithAttention(X_train_tensor.shape[1])  # assuming this class is defined correctly\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=3):\n",
    "    if cuda_available:\n",
    "        model = model.cuda()\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                if cuda_available:\n",
    "                    val_inputs, val_labels = val_inputs.cuda(), val_labels.cuda()\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs.squeeze(), val_labels).item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}, Val Loss: {val_loss}\")\n",
    "\n",
    "    return \n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU\n",
    "            all_labels.extend(labels.cpu().numpy())    # Move labels back to CPU\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.3406546711921692, Val Loss: 0.3018457219004631\n",
      "Epoch 6/50, Loss: 0.06166786327958107, Val Loss: 0.1742662334193786\n",
      "Early stopping at epoch 10\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=50, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9326334208223972, Precision: 0.9309734513274336, Recall: 0.9326241134751773, F1-Score: 0.9317980513728963\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legitimate\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def transform_and_predict(features_list, vectorizer, scaler, model):\n",
    "    # Separate the URL from the numerical features\n",
    "    url, numerical_features = features_list[0], np.array(features_list[1:])\n",
    "\n",
    "    # Transform the URL using the pre-fitted TF-IDF vectorizer\n",
    "    tfidf_features = vectorizer.transform([url]).toarray()\n",
    "\n",
    "    # Ensure numerical_features is a 2D array with a single sample\n",
    "    numerical_features = numerical_features.reshape(1, -1)\n",
    "\n",
    "    # Combine TF-IDF features with other numerical features\n",
    "    combined_features = np.hstack((tfidf_features, numerical_features))\n",
    "\n",
    "    # Standardize the features using the pre-fitted scaler\n",
    "    scaled_features = scaler.transform(combined_features)  # No need for additional brackets now\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "\n",
    "    # Predict using the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features_tensor)\n",
    "        predicted_class = (prediction.squeeze() > 0.5).float()\n",
    "        return predicted_class.item()  # Return the prediction as a Python scalar\n",
    "\n",
    "# Example usage for a single URL and its numerical features\n",
    "single_features = ['https://google.com', 18, 1, 0, 10000, 1, 18]   # URL followed by its numerical features\n",
    "prediction = transform_and_predict(single_features, vectorizer, scaler, model)\n",
    "print(\"Phishing\" if prediction == 1 else \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scaler.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the PyTorch model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "# Save the fitted TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the fitted scaler\n",
    "joblib.dump(scaler, 'standard_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
