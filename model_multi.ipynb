{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/cs181/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('dataset_phishing.csv')\n",
    "y = data['status'].apply(lambda x: 1 if x == 'phishing' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five features (excluding url-related features):\n",
      "Feature: google_index, Importance: 0.07143022622913041\n",
      "Feature: page_rank, Importance: 0.0533905471087225\n",
      "Feature: web_traffic, Importance: 0.04000302339944798\n",
      "Feature: nb_hyperlinks, Importance: 0.030772036430469583\n",
      "Feature: domain_age, Importance: 0.01800872832572652\n"
     ]
    }
   ],
   "source": [
    "all_features = data.columns.tolist()\n",
    "all_features.remove('status') \n",
    "# tf-idf embedding for url\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# extract numerical features\n",
    "numerical_features = all_features.copy()\n",
    "numerical_features.remove('url')  \n",
    "X_numerical = data[numerical_features].values\n",
    "\n",
    "# combine url with rest featrues and split\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# use random forest to rate feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, y_train)\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# find top 20 important features in url\n",
    "url_feature_importances = feature_importances[:len(tfidf_feature_names)]\n",
    "url_features_with_importance = list(zip(tfidf_feature_names, url_feature_importances))\n",
    "top_ten_url_features = sorted(url_features_with_importance, key=lambda x: x[1], reverse=True)[:20]\n",
    "# print(\"Top 20 URL-related features:\")\n",
    "# for feature, importance in top_ten_url_features:\n",
    "#     print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "# find top 5 important features except url\n",
    "remaining_feature_importances = feature_importances[len(tfidf_feature_names):]\n",
    "remaining_feature_names = numerical_features\n",
    "remaining_features_with_importance = list(zip(remaining_feature_names, remaining_feature_importances))\n",
    "top_five_features = sorted(remaining_features_with_importance, key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top five features (excluding url-related features):\")\n",
    "for feature, importance in top_five_features:\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature and label extraction, including additional numerical features\n",
    "selected_features = [ 'length_url', 'nb_dots', 'https_token', 'nb_subdomains', 'google_index', 'nb_hyperlinks']  # Add other selected feature column names\n",
    "# selected_features = [item[0] for item in top_five_features]\n",
    "X_numerical = data[selected_features].values  # Extract numerical features\n",
    "\n",
    "# Character n-gram TF-IDF vectorization for the 'url' feature\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(2, 5), max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(data['url']).toarray()\n",
    "\n",
    "# top_features_names = [feature[0] for feature in top_ten_url_features]\n",
    "# feature_indices = [list(tfidf_feature_names).index(feature) for feature in top_features_names]\n",
    "# feature_values = X_tfidf[:, feature_indices]\n",
    "\n",
    "# Combine TF-IDF features with numerical features\n",
    "X_combined = np.hstack((X_tfidf, X_numerical))\n",
    "# X_combined = np.hstack((feature_values, X_numerical))\n",
    "\n",
    "# Fit the scaler only on the training data and transform both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.proj = nn.Linear(feature_dim, 64)\n",
    "        self.out = nn.Linear(64, feature_dim)  # Ensure output dimension matches feature_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        eij = self.proj(x)\n",
    "        eij = torch.tanh(eij)\n",
    "        eij = self.out(eij)\n",
    "        a = torch.softmax(eij, dim=1)\n",
    "        weighted_input = x * a\n",
    "        return weighted_input  # Return the weighted input for further processing\n",
    "\n",
    "class AdvancedURLNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(AdvancedURLNetWithAttention, self).__init__()\n",
    "        # Increased the complexity of the first layer and added a second attention layer\n",
    "        self.fc1 = nn.Linear(num_features, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)  # Reduced dropout\n",
    "        self.attention1 = Attention(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.attention2 = Attention(512)  # New attention layer\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.attention1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.attention2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "model = AdvancedURLNetWithAttention(X_train_tensor.shape[1])  # assuming this class is defined correctly\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    if cuda_available:\n",
    "        model = model.cuda()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            if cuda_available:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Move predictions back to CPU\n",
    "            all_labels.extend(labels.cpu().numpy())    # Move labels back to CPU\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.31001028418540955\n",
      "Epoch 6/100, Loss: 0.02753426320850849\n",
      "Epoch 11/100, Loss: 0.05657140165567398\n",
      "Epoch 16/100, Loss: 0.001671155565418303\n",
      "Epoch 21/100, Loss: 0.006874675862491131\n",
      "Epoch 26/100, Loss: 0.03208216652274132\n",
      "Epoch 31/100, Loss: 0.0008164627361111343\n",
      "Epoch 36/100, Loss: 0.0008875590865500271\n",
      "Epoch 41/100, Loss: 7.306803308892995e-05\n",
      "Epoch 46/100, Loss: 0.018874378874897957\n",
      "Epoch 51/100, Loss: 0.0003280696109868586\n",
      "Epoch 56/100, Loss: 0.000388886226573959\n",
      "Epoch 61/100, Loss: 3.3187818189617246e-05\n",
      "Epoch 66/100, Loss: 0.00013747272896580398\n",
      "Epoch 71/100, Loss: 0.01297763455659151\n",
      "Epoch 76/100, Loss: 0.0003326671721879393\n",
      "Epoch 81/100, Loss: 0.00012457469711080194\n",
      "Epoch 86/100, Loss: 0.0004040691419504583\n",
      "Epoch 91/100, Loss: 8.636554412078112e-05\n",
      "Epoch 96/100, Loss: 6.143678911030293e-05\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9444444444444444, Precision: 0.9481216457960644, Recall: 0.9388839681133747, F1-Score: 0.9434801958166444\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "\n",
    "def transform_and_predict(features_list, vectorizer, scaler, model):\n",
    "    # Separate the URL from the numerical features\n",
    "    url, numerical_features = features_list[0], np.array(features_list[1:])\n",
    "\n",
    "    # Transform the URL using the pre-fitted TF-IDF vectorizer\n",
    "    tfidf_features = vectorizer.transform([url]).toarray()\n",
    "\n",
    "    # Ensure numerical_features is a 2D array with a single sample\n",
    "    numerical_features = numerical_features.reshape(1, -1)\n",
    "\n",
    "    # Combine TF-IDF features with other numerical features\n",
    "    combined_features = np.hstack((tfidf_features, numerical_features))\n",
    "\n",
    "    # Standardize the features using the pre-fitted scaler\n",
    "    scaled_features = scaler.transform(combined_features)  # No need for additional brackets now\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    features_tensor = torch.tensor(scaled_features, dtype=torch.float32)\n",
    "\n",
    "    # Predict using the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model(features_tensor)\n",
    "        predicted_class = (prediction.squeeze() > 0.5).float()\n",
    "        return predicted_class.item()  # Return the prediction as a Python scalar\n",
    "\n",
    "# Example usage for a single URL and its numerical features\n",
    "single_features = [\"http://www.crestonwood.com/router.php\", 35, 3, 1, 2,4,5]  # URL followed by its numerical features\n",
    "prediction = transform_and_predict(single_features, vectorizer, scaler, model)\n",
    "print(\"Phishing\" if prediction == 1 else \"Legitimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['standard_scaler.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the PyTorch model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_state_dict.pth')\n",
    "\n",
    "# Save the fitted TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "\n",
    "# Save the fitted scaler\n",
    "joblib.dump(scaler, 'standard_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
